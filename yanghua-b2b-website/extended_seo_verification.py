#!/usr/bin/env python3
"""
æ‰©å±•é¡µé¢ SEO éªŒè¯è„šæœ¬
éªŒè¯ projectsã€solutionsã€servicesã€contact é¡µé¢çš„ hreflang å’Œ canonical æ ‡ç­¾é…ç½®
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import sys

# ç¯å¢ƒé…ç½®
PRODUCTION_BASE_URL = "https://www.yhflexiblebusbar.com"
LOCAL_BASE_URL = "http://localhost:3003"

# æ‰©å±•é¡µé¢é…ç½®
EXTENDED_PAGES = [
    {"path": "/en/projects", "name": "è‹±æ–‡é¡¹ç›®é¡µ", "type": "projects", "lang": "en"},
    {"path": "/es/proyectos", "name": "è¥¿ç­ç‰™è¯­é¡¹ç›®é¡µ", "type": "projects", "lang": "es"},
    {"path": "/en/solutions", "name": "è‹±æ–‡è§£å†³æ–¹æ¡ˆé¡µ", "type": "solutions", "lang": "en"},
    {"path": "/es/soluciones", "name": "è¥¿ç­ç‰™è¯­è§£å†³æ–¹æ¡ˆé¡µ", "type": "solutions", "lang": "es"},
    {"path": "/en/services", "name": "è‹±æ–‡æœåŠ¡é¡µ", "type": "services", "lang": "en"},
    {"path": "/es/servicios", "name": "è¥¿ç­ç‰™è¯­æœåŠ¡é¡µ", "type": "services", "lang": "es"},
    {"path": "/en/contact", "name": "è‹±æ–‡è”ç³»é¡µ", "type": "contact", "lang": "en"},
    {"path": "/es/contacto", "name": "è¥¿ç­ç‰™è¯­è”ç³»é¡µ", "type": "contact", "lang": "es"},
]

def get_page_content(url):
    """è·å–é¡µé¢å†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"âŒ è·å–é¡µé¢å¤±è´¥ {url}: {e}")
        return None

def extract_seo_tags(html_content):
    """æå– SEO ç›¸å…³æ ‡ç­¾"""
    if not html_content:
        return None
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æå– canonical æ ‡ç­¾
    canonical = soup.find('link', {'rel': 'canonical'})
    canonical_url = canonical.get('href') if canonical else None
    
    # æå– hreflang æ ‡ç­¾
    hreflang_tags = soup.find_all('link', {'rel': 'alternate', 'hreflang': True})
    hreflang_data = []
    for tag in hreflang_tags:
        hreflang_data.append({
            'hreflang': tag.get('hreflang'),
            'href': tag.get('href')
        })
    
    # æå–åŸºæœ¬ meta æ ‡ç­¾
    title = soup.find('title')
    title_text = title.get_text().strip() if title else None
    
    meta_description = soup.find('meta', {'name': 'description'})
    description = meta_description.get('content') if meta_description else None
    
    # æå– Open Graph æ ‡ç­¾
    og_url = soup.find('meta', {'property': 'og:url'})
    og_url_content = og_url.get('content') if og_url else None
    
    og_title = soup.find('meta', {'property': 'og:title'})
    og_title_content = og_title.get('content') if og_title else None
    
    return {
        'canonical': canonical_url,
        'hreflang': hreflang_data,
        'title': title_text,
        'description': description,
        'og_url': og_url_content,
        'og_title': og_title_content
    }

def validate_seo_tags(seo_data, page_info, base_url):
    """éªŒè¯ SEO æ ‡ç­¾çš„æ­£ç¡®æ€§"""
    issues = []
    warnings = []
    
    if not seo_data:
        issues.append("æ— æ³•æå– SEO æ•°æ®")
        return issues, warnings
    
    # éªŒè¯ canonical URL
    expected_canonical = f"{base_url}{page_info['path']}"
    if not seo_data['canonical']:
        issues.append("ç¼ºå°‘ canonical æ ‡ç­¾")
    elif seo_data['canonical'] != expected_canonical:
        issues.append(f"Canonical URL ä¸æ­£ç¡®: æœŸæœ› {expected_canonical}, å®é™… {seo_data['canonical']}")
    
    # éªŒè¯ hreflang æ ‡ç­¾
    if not seo_data['hreflang']:
        issues.append("ç¼ºå°‘ hreflang æ ‡ç­¾")
    else:
        hreflang_langs = [tag['hreflang'] for tag in seo_data['hreflang']]
        
        # æ£€æŸ¥å¿…éœ€çš„è¯­è¨€æ ‡ç­¾
        required_langs = ['en', 'es', 'x-default']
        missing_langs = [lang for lang in required_langs if lang not in hreflang_langs]
        if missing_langs:
            issues.append(f"ç¼ºå°‘ hreflang è¯­è¨€æ ‡ç­¾: {', '.join(missing_langs)}")
        
        # æ£€æŸ¥ hreflang URL æ ¼å¼
        for tag in seo_data['hreflang']:
            if not tag['href'].startswith('http'):
                issues.append(f"Hreflang URL ä¸æ˜¯ç»å¯¹è·¯å¾„: {tag['hreflang']} -> {tag['href']}")
    
    # éªŒè¯é¡µé¢æ ‡é¢˜
    if not seo_data['title']:
        warnings.append("ç¼ºå°‘é¡µé¢æ ‡é¢˜")
    elif len(seo_data['title']) < 30:
        warnings.append("é¡µé¢æ ‡é¢˜å¯èƒ½è¿‡çŸ­")
    elif len(seo_data['title']) > 60:
        warnings.append("é¡µé¢æ ‡é¢˜å¯èƒ½è¿‡é•¿")
    
    # éªŒè¯ meta æè¿°
    if not seo_data['description']:
        warnings.append("ç¼ºå°‘ meta æè¿°")
    elif len(seo_data['description']) < 120:
        warnings.append("Meta æè¿°å¯èƒ½è¿‡çŸ­")
    elif len(seo_data['description']) > 160:
        warnings.append("Meta æè¿°å¯èƒ½è¿‡é•¿")
    
    return issues, warnings

def check_page_seo(base_url, page_info):
    """æ£€æŸ¥å•ä¸ªé¡µé¢çš„ SEO é…ç½®"""
    url = f"{base_url}{page_info['path']}"
    print(f"   ğŸ” æ£€æŸ¥: {url}")
    
    html_content = get_page_content(url)
    seo_data = extract_seo_tags(html_content)
    issues, warnings = validate_seo_tags(seo_data, page_info, base_url)
    
    return {
        'url': url,
        'page_info': page_info,
        'seo_data': seo_data,
        'issues': issues,
        'warnings': warnings,
        'status': 'success' if not issues else 'failed'
    }

def compare_environments(page_info):
    """å¯¹æ¯”ç”Ÿäº§ç¯å¢ƒå’Œæœ¬åœ°ç¯å¢ƒçš„ SEO é…ç½®"""
    print(f"ğŸ”„ å¯¹æ¯”ç¯å¢ƒ: {page_info['name']}")
    
    # æ£€æŸ¥ç”Ÿäº§ç¯å¢ƒ
    prod_result = check_page_seo(PRODUCTION_BASE_URL, page_info)
    
    # æ£€æŸ¥æœ¬åœ°ç¯å¢ƒ
    local_result = check_page_seo(LOCAL_BASE_URL, page_info)
    
    # å¯¹æ¯”ç»“æœ
    comparison = {
        'page_name': page_info['name'],
        'production': prod_result,
        'local': local_result,
        'differences': [],
        'environment_issues': []
    }
    
    # æ£€æŸ¥ç¯å¢ƒå·®å¼‚
    if prod_result['status'] == 'success' and local_result['status'] == 'success':
        prod_seo = prod_result['seo_data']
        local_seo = local_result['seo_data']
        
        # å¯¹æ¯” canonical URL
        if prod_seo['canonical'] != local_seo['canonical']:
            comparison['differences'].append({
                'field': 'canonical',
                'production': prod_seo['canonical'],
                'local': local_seo['canonical']
            })
        
        # å¯¹æ¯” hreflang æ ‡ç­¾
        prod_hreflang = {tag['hreflang']: tag['href'] for tag in prod_seo['hreflang']}
        local_hreflang = {tag['hreflang']: tag['href'] for tag in local_seo['hreflang']}
        
        if prod_hreflang != local_hreflang:
            comparison['differences'].append({
                'field': 'hreflang',
                'production': prod_hreflang,
                'local': local_hreflang
            })
    
    # è®°å½•ç¯å¢ƒé—®é¢˜
    if prod_result['status'] == 'failed':
        comparison['environment_issues'].append(f"ç”Ÿäº§ç¯å¢ƒé—®é¢˜: {', '.join(prod_result['issues'])}")
    
    if local_result['status'] == 'failed':
        comparison['environment_issues'].append(f"æœ¬åœ°ç¯å¢ƒé—®é¢˜: {', '.join(local_result['issues'])}")
    
    return comparison

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ‰©å±•é¡µé¢ SEO éªŒè¯")
    print(f"ğŸŒ ç”Ÿäº§ç¯å¢ƒ: {PRODUCTION_BASE_URL}")
    print(f"ğŸ  æœ¬åœ°ç¯å¢ƒ: {LOCAL_BASE_URL}")
    print(f"â° éªŒè¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“„ éªŒè¯é¡µé¢æ•°: {len(EXTENDED_PAGES)}")
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'production_base_url': PRODUCTION_BASE_URL,
        'local_base_url': LOCAL_BASE_URL,
        'pages_checked': len(EXTENDED_PAGES),
        'results': {},
        'summary': {
            'total_pages': len(EXTENDED_PAGES),
            'successful_checks': 0,
            'failed_checks': 0,
            'pages_with_issues': 0,
            'pages_with_warnings': 0,
            'environment_differences': 0
        }
    }
    
    # æ£€æŸ¥æ¯ä¸ªé¡µé¢
    for page_info in EXTENDED_PAGES:
        print(f"\nğŸ“‹ æ£€æŸ¥é¡µé¢: {page_info['name']} ({page_info['path']})")
        
        try:
            # å¯¹æ¯”ä¸¤ä¸ªç¯å¢ƒ
            comparison = compare_environments(page_info)
            results['results'][page_info['path']] = comparison
            
            # æ›´æ–°ç»Ÿè®¡
            if comparison['production']['status'] == 'success' and comparison['local']['status'] == 'success':
                results['summary']['successful_checks'] += 1
                
                if comparison['differences']:
                    results['summary']['environment_differences'] += 1
                    print(f"   âš ï¸  å‘ç° {len(comparison['differences'])} ä¸ªç¯å¢ƒå·®å¼‚")
                else:
                    print("   âœ… ä¸¤ä¸ªç¯å¢ƒé…ç½®ä¸€è‡´")
            else:
                results['summary']['failed_checks'] += 1
                print("   âŒ æ£€æŸ¥å¤±è´¥")
            
            # ç»Ÿè®¡é—®é¢˜å’Œè­¦å‘Š
            prod_issues = len(comparison['production'].get('issues', []))
            local_issues = len(comparison['local'].get('issues', []))
            if prod_issues > 0 or local_issues > 0:
                results['summary']['pages_with_issues'] += 1
            
            prod_warnings = len(comparison['production'].get('warnings', []))
            local_warnings = len(comparison['local'].get('warnings', []))
            if prod_warnings > 0 or local_warnings > 0:
                results['summary']['pages_with_warnings'] += 1
                
        except Exception as e:
            print(f"âŒ æ£€æŸ¥é¡µé¢ {page_info['name']} æ—¶å‡ºé”™: {e}")
            results['results'][page_info['path']] = {
                'page_name': page_info['name'],
                'error': str(e)
            }
            results['summary']['failed_checks'] += 1
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    output_file = f"extended_seo_verification_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š éªŒè¯å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
    summary = results['summary']
    print(f"\nğŸ“ˆ éªŒè¯æ‘˜è¦:")
    print(f"   - æ€»é¡µé¢æ•°: {summary['total_pages']}")
    print(f"   - æˆåŠŸæ£€æŸ¥: {summary['successful_checks']}")
    print(f"   - æ£€æŸ¥å¤±è´¥: {summary['failed_checks']}")
    print(f"   - æœ‰é—®é¢˜çš„é¡µé¢: {summary['pages_with_issues']}")
    print(f"   - æœ‰è­¦å‘Šçš„é¡µé¢: {summary['pages_with_warnings']}")
    print(f"   - ç¯å¢ƒå·®å¼‚é¡µé¢: {summary['environment_differences']}")
    
    # æŒ‰é¡µé¢ç±»å‹åˆ†ç»„ç»Ÿè®¡
    page_types = {}
    for page_info in EXTENDED_PAGES:
        page_type = page_info['type']
        if page_type not in page_types:
            page_types[page_type] = {'total': 0, 'success': 0}
        page_types[page_type]['total'] += 1
        
        result = results['results'].get(page_info['path'])
        if result and result.get('production', {}).get('status') == 'success' and result.get('local', {}).get('status') == 'success':
            page_types[page_type]['success'] += 1
    
    print(f"\nğŸ“Š æŒ‰é¡µé¢ç±»å‹ç»Ÿè®¡:")
    for page_type, stats in page_types.items():
        success_rate = (stats['success'] / stats['total']) * 100 if stats['total'] > 0 else 0
        print(f"   - {page_type}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)")
    
    # æœ€ç»ˆçŠ¶æ€
    if summary['failed_checks'] == 0 and summary['environment_differences'] == 0:
        print("\nğŸ‰ æ‰€æœ‰æ‰©å±•é¡µé¢çš„ SEO é…ç½®éªŒè¯é€šè¿‡ï¼")
        return 0
    elif summary['failed_checks'] > 0:
        print(f"\nâŒ {summary['failed_checks']} ä¸ªé¡µé¢æ£€æŸ¥å¤±è´¥ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")
        return 1
    else:
        print(f"\nâš ï¸  å‘ç° {summary['environment_differences']} ä¸ªé¡µé¢å­˜åœ¨ç¯å¢ƒå·®å¼‚")
        return 0

if __name__ == "__main__":
    sys.exit(main())