#!/usr/bin/env python3
"""
ç”Ÿäº§ç¯å¢ƒæ–‡ç« é¡µé¢ SEO æ ‡ç­¾éªŒè¯è„šæœ¬
æ£€æŸ¥å®é™…æ–‡ç« é¡µé¢çš„ hreflang å’Œ canonical æ ‡ç­¾
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import sys
import argparse

# ç”Ÿäº§ç¯å¢ƒé…ç½®
PRODUCTION_BASE_URL = "https://www.yhflexiblebusbar.com"

def get_articles_from_sitemap(max_count: int = 50):
    """ä» sitemap.xml è·å–æ–‡ç« é“¾æ¥ï¼ˆå¯è®¾ç½®æœ€å¤§æ•°é‡ï¼‰"""
    try:
        sitemap_url = f"{PRODUCTION_BASE_URL}/sitemap.xml"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        res = requests.get(sitemap_url, headers=headers, timeout=30)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'xml')
        locs = soup.find_all('loc')
        urls = []
        for loc in locs:
            u = (loc.get_text() or '').strip()
            if '/articles/' in u:
                urls.append(u)
        unique = list(dict.fromkeys(urls))[:max_count]
        if unique:
            print(f"é€šè¿‡ sitemap.xml è·å–åˆ° {len(unique)} ç¯‡æ–‡ç« é“¾æ¥")
        return unique
    except Exception as e:
        print(f"âš ï¸ ä» sitemap.xml è·å–æ–‡ç« å¤±è´¥: {e}")
        return []

def get_articles_from_local_data(max_count: int = 50):
    """ä»æœ¬åœ°å¯¼å‡ºçš„ article_data.json è·å–æ–‡ç« é“¾æ¥ï¼ˆä½œä¸ºå…œåº•ï¼Œå¯è®¾ç½®æœ€å¤§æ•°é‡ï¼‰"""
    try:
        path = 'yanghua-b2b-website/yanghua-b2b-website/article_data.json'
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        items = data.get('data') or []
        urls = []
        for item in items:
            slug = item.get('slug')
            locale = item.get('locale') or 'en'
            if slug:
                urls.append(f"{PRODUCTION_BASE_URL}/{locale}/articles/{slug}")
        unique = list(dict.fromkeys(urls))[:max_count]
        if unique:
            print(f"é€šè¿‡æœ¬åœ° article_data.json è·å–åˆ° {len(unique)} ç¯‡æ–‡ç« é“¾æ¥")
        return unique
    except Exception as e:
        print(f"âš ï¸ ä»æœ¬åœ°æ–‡ç« æ•°æ®è·å–å¤±è´¥: {e}")
        return []

def get_articles_list(max_count: int = 50):
    """è·å–æ–‡ç« åˆ—è¡¨ï¼ˆå¤šæ¥æºå…œåº•ï¼Œå¯è®¾ç½®æœ€å¤§æ•°é‡ï¼‰"""
    # å…ˆå°è¯•ä»è‹±æ–‡æ–‡ç« åˆ—è¡¨é¡µæŠ“å–
    try:
        url = f"{PRODUCTION_BASE_URL}/en/articles"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
        article_links = []
        links = soup.find_all('a', href=True)
        for link in links:
            href = link.get('href')
            if href and '/articles/' in href:
                if href.startswith('/'):
                    href = PRODUCTION_BASE_URL + href
                article_links.append(href)

        unique_links = list(dict.fromkeys(article_links))[:max_count]
        if unique_links:
            print(f"é€šè¿‡åˆ—è¡¨é¡µæ‰¾åˆ° {len(unique_links)} ç¯‡æ–‡ç« è¿›è¡Œæ£€æŸ¥")
            return unique_links
    except Exception as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè·å–å¤±è´¥: {e}")

    # åˆ—è¡¨é¡µæœªè·å–åˆ°ï¼Œå°è¯• sitemap
    sitemap_articles = get_articles_from_sitemap(max_count)
    if sitemap_articles:
        return sitemap_articles

    # ä»ä¸ºç©ºï¼Œå°è¯•æœ¬åœ°å¯¼å‡ºçš„æ–‡ç« æ•°æ®
    local_articles = get_articles_from_local_data(max_count)
    if local_articles:
        return local_articles

    # æœ€åå…œåº•ï¼šä½¿ç”¨ä¸€äº›å¸¸è§çš„æ–‡ç« è·¯å¾„
    print("âŒ æœªä»é¡µé¢ã€sitemap æˆ–æœ¬åœ°æ•°æ®è·å–åˆ°æ–‡ç« ï¼Œä½¿ç”¨å†…ç½®å…œåº•åˆ—è¡¨")
    return [
        f"{PRODUCTION_BASE_URL}/en/articles/flexible-busbar-technology",
        f"{PRODUCTION_BASE_URL}/en/articles/data-center-power-solutions",
        f"{PRODUCTION_BASE_URL}/en/articles/ev-charging-infrastructure"
    ]
    
def get_page_content(url):
    """è·å–é¡µé¢å†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"âŒ è·å–é¡µé¢å¤±è´¥ {url}: {e}")
        return None

def extract_seo_tags(html_content):
    """æå– SEO ç›¸å…³æ ‡ç­¾"""
    if not html_content:
        return None
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æå– canonical æ ‡ç­¾
    canonical = soup.find('link', {'rel': 'canonical'})
    canonical_url = canonical.get('href') if canonical else None
    
    # æå– hreflang æ ‡ç­¾
    hreflang_tags = soup.find_all('link', {'rel': 'alternate', 'hreflang': True})
    hreflang_data = []
    for tag in hreflang_tags:
        hreflang_data.append({
            'hreflang': tag.get('hreflang'),
            'href': tag.get('href')
        })
    
    # æå–åŸºæœ¬ meta æ ‡ç­¾
    title = soup.find('title')
    title_text = title.get_text().strip() if title else None
    
    meta_description = soup.find('meta', {'name': 'description'})
    description = meta_description.get('content') if meta_description else None
    
    # æå– Open Graph æ ‡ç­¾
    og_url = soup.find('meta', {'property': 'og:url'})
    og_url_content = og_url.get('content') if og_url else None
    
    # æå–æ–‡ç« ç‰¹å®šçš„ meta æ ‡ç­¾
    og_type = soup.find('meta', {'property': 'og:type'})
    og_type_content = og_type.get('content') if og_type else None
    
    article_author = soup.find('meta', {'name': 'author'})
    author = article_author.get('content') if article_author else None
    
    return {
        'canonical': canonical_url,
        'hreflang': hreflang_data,
        'title': title_text,
        'description': description,
        'og_url': og_url_content,
        'og_type': og_type_content,
        'author': author
    }

def check_article_seo(article_url):
    """æ£€æŸ¥å•ç¯‡æ–‡ç« çš„ SEO é…ç½®"""
    print(f"\nğŸ” æ£€æŸ¥æ–‡ç« : {article_url}")
    
    html_content = get_page_content(article_url)
    if not html_content:
        return None
    
    seo_data = extract_seo_tags(html_content)
    if not seo_data:
        print("âŒ æ— æ³•æå– SEO æ ‡ç­¾")
        return None
    
    # æ£€æŸ¥ç»“æœ
    print(f"ğŸ“„ æ–‡ç« æ ‡é¢˜: {seo_data['title']}")
    print(f"ğŸ“ Meta æè¿°: {seo_data['description'][:100] if seo_data['description'] else 'None'}...")
    
    if seo_data['author']:
        print(f"ğŸ‘¤ ä½œè€…: {seo_data['author']}")
    
    if seo_data['og_type']:
        print(f"ğŸ“° å†…å®¹ç±»å‹: {seo_data['og_type']}")
    
    # Canonical æ£€æŸ¥
    if seo_data['canonical']:
        print(f"ğŸ”— Canonical URL: {seo_data['canonical']}")
        if article_url in seo_data['canonical'] or seo_data['canonical'] == article_url:
            print("âœ… Canonical URL æ­£ç¡®")
        else:
            print("âš ï¸  Canonical URL å¯èƒ½ä¸åŒ¹é…")
    else:
        print("âŒ ç¼ºå°‘ Canonical æ ‡ç­¾")
    
    # Hreflang æ£€æŸ¥
    if seo_data['hreflang']:
        print(f"ğŸŒ Hreflang æ ‡ç­¾æ•°é‡: {len(seo_data['hreflang'])}")
        for hreflang in seo_data['hreflang']:
            print(f"   - {hreflang['hreflang']}: {hreflang['href']}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è‹±æ–‡å’Œè¥¿ç­ç‰™è¯­
        languages = [tag['hreflang'] for tag in seo_data['hreflang']]
        if 'en' in languages and 'es' in languages:
            print("âœ… åŒ…å«è‹±æ–‡å’Œè¥¿ç­ç‰™è¯­ hreflang")
        elif 'en' in languages:
            print("âš ï¸  åªåŒ…å«è‹±æ–‡ hreflangï¼Œå¯èƒ½ç¼ºå°‘è¥¿ç­ç‰™è¯­ç‰ˆæœ¬")
        else:
            print("âš ï¸  hreflang é…ç½®å¯èƒ½ä¸å®Œæ•´")
    else:
        print("âŒ ç¼ºå°‘ Hreflang æ ‡ç­¾")
    
    # Open Graph URL æ£€æŸ¥
    if seo_data['og_url']:
        print(f"ğŸ“± OG URL: {seo_data['og_url']}")
    
    return seo_data

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ£€æŸ¥ç”Ÿäº§ç¯å¢ƒæ–‡ç« é¡µé¢ SEO æ ‡ç­¾é…ç½®")
    print(f"ğŸŒ ç”Ÿäº§ç¯å¢ƒ: {PRODUCTION_BASE_URL}")
    print(f"â° æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='ç”Ÿäº§æ–‡ç«  SEO æ ‡ç­¾æ£€æŸ¥')
    parser.add_argument('urls', nargs='*', help='è¦æ£€æŸ¥çš„æ–‡ç«  URL åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--limit', type=int, default=10, help='æœ€å¤šæ£€æŸ¥çš„æ–‡ç« æ•°é‡ï¼ˆé»˜è®¤ 10ï¼‰')
    parser.add_argument('--source', choices=['auto', 'sitemap', 'list', 'local'], default='auto', help='æ–‡ç« æ¥æºï¼ˆé»˜è®¤è‡ªåŠ¨ï¼‰')
    args = parser.parse_args()

    # å¦‚æœæä¾›äº†å‘½ä»¤è¡Œ URLï¼Œä¼˜å…ˆä½¿ç”¨
    if args.urls:
        print(f"ğŸ§ª ä½¿ç”¨å‘½ä»¤è¡Œæä¾›çš„ {len(args.urls)} ä¸ª URL è¿›è¡Œæ£€æŸ¥")
        article_urls = args.urls
    else:
        # æ ¹æ®æ¥æºé€‰æ‹©ç­–ç•¥è·å–æ–‡ç« åˆ—è¡¨
        if args.source == 'sitemap':
            article_urls = get_articles_from_sitemap(args.limit)
        elif args.source == 'list':
            article_urls = get_articles_list(args.limit)
        elif args.source == 'local':
            article_urls = get_articles_from_local_data(args.limit)
        else:
            article_urls = get_articles_list(args.limit)
    
    if not article_urls:
        print("âŒ æ— æ³•è·å–æ–‡ç« åˆ—è¡¨")
        return
    
    results = {}
    
    for article_url in article_urls:
        try:
            seo_data = check_article_seo(article_url)
            if seo_data:
                results[article_url] = {
                    'seo_data': seo_data,
                    'status': 'success'
                }
            else:
                results[article_url] = {
                    'status': 'failed'
                }
        except Exception as e:
            print(f"âŒ æ£€æŸ¥æ–‡ç«  {article_url} æ—¶å‡ºé”™: {e}")
            results[article_url] = {
                'status': 'error',
                'error': str(e)
            }
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    output_file = f"production_articles_seo_check_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š æ£€æŸ¥å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ç”Ÿæˆæ‘˜è¦
    successful_checks = sum(1 for r in results.values() if r['status'] == 'success')
    total_checks = len(results)
    
    print(f"\nğŸ“ˆ æ£€æŸ¥æ‘˜è¦:")
    print(f"   - æ€»æ–‡ç« æ•°: {total_checks}")
    print(f"   - æˆåŠŸæ£€æŸ¥: {successful_checks}")
    print(f"   - å¤±è´¥æ£€æŸ¥: {total_checks - successful_checks}")
    
    if successful_checks == total_checks:
        print("ğŸ‰ æ‰€æœ‰æ–‡ç« æ£€æŸ¥å®Œæˆï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æ–‡ç« æ£€æŸ¥å¤±è´¥ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æ—¥å¿—")

if __name__ == "__main__":
    main()