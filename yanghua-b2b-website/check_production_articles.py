#!/usr/bin/env python3
"""
ç”Ÿäº§ç¯å¢ƒæ–‡ç« é¡µé¢ SEO æ ‡ç­¾éªŒè¯è„šæœ¬
æ£€æŸ¥å®é™…æ–‡ç« é¡µé¢çš„ hreflang å’Œ canonical æ ‡ç­¾
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import sys

# ç”Ÿäº§ç¯å¢ƒé…ç½®
PRODUCTION_BASE_URL = "https://www.yhflexiblebusbar.com"

def get_articles_list():
    """è·å–æ–‡ç« åˆ—è¡¨"""
    try:
        # å…ˆè·å–è‹±æ–‡æ–‡ç« åˆ—è¡¨é¡µé¢
        url = f"{PRODUCTION_BASE_URL}/en/articles"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
        article_links = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ–‡ç« é“¾æ¥
        links = soup.find_all('a', href=True)
        for link in links:
            href = link.get('href')
            if href and '/articles/' in href:
                if href.startswith('/'):
                    href = PRODUCTION_BASE_URL + href
                article_links.append(href)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_links = list(set(article_links))[:5]  # åªæ£€æŸ¥å‰5ç¯‡æ–‡ç« 
        
        print(f"æ‰¾åˆ° {len(unique_links)} ç¯‡æ–‡ç« è¿›è¡Œæ£€æŸ¥")
        return unique_links
        
    except Exception as e:
        print(f"âŒ è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
        # å¦‚æœæ— æ³•è·å–æ–‡ç« åˆ—è¡¨ï¼Œä½¿ç”¨ä¸€äº›å¸¸è§çš„æ–‡ç« è·¯å¾„
        return [
            f"{PRODUCTION_BASE_URL}/en/articles/flexible-busbar-technology",
            f"{PRODUCTION_BASE_URL}/en/articles/data-center-power-solutions",
            f"{PRODUCTION_BASE_URL}/en/articles/ev-charging-infrastructure"
        ]

def get_page_content(url):
    """è·å–é¡µé¢å†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"âŒ è·å–é¡µé¢å¤±è´¥ {url}: {e}")
        return None

def extract_seo_tags(html_content):
    """æå– SEO ç›¸å…³æ ‡ç­¾"""
    if not html_content:
        return None
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æå– canonical æ ‡ç­¾
    canonical = soup.find('link', {'rel': 'canonical'})
    canonical_url = canonical.get('href') if canonical else None
    
    # æå– hreflang æ ‡ç­¾
    hreflang_tags = soup.find_all('link', {'rel': 'alternate', 'hreflang': True})
    hreflang_data = []
    for tag in hreflang_tags:
        hreflang_data.append({
            'hreflang': tag.get('hreflang'),
            'href': tag.get('href')
        })
    
    # æå–åŸºæœ¬ meta æ ‡ç­¾
    title = soup.find('title')
    title_text = title.get_text().strip() if title else None
    
    meta_description = soup.find('meta', {'name': 'description'})
    description = meta_description.get('content') if meta_description else None
    
    # æå– Open Graph æ ‡ç­¾
    og_url = soup.find('meta', {'property': 'og:url'})
    og_url_content = og_url.get('content') if og_url else None
    
    # æå–æ–‡ç« ç‰¹å®šçš„ meta æ ‡ç­¾
    og_type = soup.find('meta', {'property': 'og:type'})
    og_type_content = og_type.get('content') if og_type else None
    
    article_author = soup.find('meta', {'name': 'author'})
    author = article_author.get('content') if article_author else None
    
    return {
        'canonical': canonical_url,
        'hreflang': hreflang_data,
        'title': title_text,
        'description': description,
        'og_url': og_url_content,
        'og_type': og_type_content,
        'author': author
    }

def check_article_seo(article_url):
    """æ£€æŸ¥å•ç¯‡æ–‡ç« çš„ SEO é…ç½®"""
    print(f"\nğŸ” æ£€æŸ¥æ–‡ç« : {article_url}")
    
    html_content = get_page_content(article_url)
    if not html_content:
        return None
    
    seo_data = extract_seo_tags(html_content)
    if not seo_data:
        print("âŒ æ— æ³•æå– SEO æ ‡ç­¾")
        return None
    
    # æ£€æŸ¥ç»“æœ
    print(f"ğŸ“„ æ–‡ç« æ ‡é¢˜: {seo_data['title']}")
    print(f"ğŸ“ Meta æè¿°: {seo_data['description'][:100] if seo_data['description'] else 'None'}...")
    
    if seo_data['author']:
        print(f"ğŸ‘¤ ä½œè€…: {seo_data['author']}")
    
    if seo_data['og_type']:
        print(f"ğŸ“° å†…å®¹ç±»å‹: {seo_data['og_type']}")
    
    # Canonical æ£€æŸ¥
    if seo_data['canonical']:
        print(f"ğŸ”— Canonical URL: {seo_data['canonical']}")
        if article_url in seo_data['canonical'] or seo_data['canonical'] == article_url:
            print("âœ… Canonical URL æ­£ç¡®")
        else:
            print("âš ï¸  Canonical URL å¯èƒ½ä¸åŒ¹é…")
    else:
        print("âŒ ç¼ºå°‘ Canonical æ ‡ç­¾")
    
    # Hreflang æ£€æŸ¥
    if seo_data['hreflang']:
        print(f"ğŸŒ Hreflang æ ‡ç­¾æ•°é‡: {len(seo_data['hreflang'])}")
        for hreflang in seo_data['hreflang']:
            print(f"   - {hreflang['hreflang']}: {hreflang['href']}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è‹±æ–‡å’Œè¥¿ç­ç‰™è¯­
        languages = [tag['hreflang'] for tag in seo_data['hreflang']]
        if 'en' in languages and 'es' in languages:
            print("âœ… åŒ…å«è‹±æ–‡å’Œè¥¿ç­ç‰™è¯­ hreflang")
        elif 'en' in languages:
            print("âš ï¸  åªåŒ…å«è‹±æ–‡ hreflangï¼Œå¯èƒ½ç¼ºå°‘è¥¿ç­ç‰™è¯­ç‰ˆæœ¬")
        else:
            print("âš ï¸  hreflang é…ç½®å¯èƒ½ä¸å®Œæ•´")
    else:
        print("âŒ ç¼ºå°‘ Hreflang æ ‡ç­¾")
    
    # Open Graph URL æ£€æŸ¥
    if seo_data['og_url']:
        print(f"ğŸ“± OG URL: {seo_data['og_url']}")
    
    return seo_data

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ£€æŸ¥ç”Ÿäº§ç¯å¢ƒæ–‡ç« é¡µé¢ SEO æ ‡ç­¾é…ç½®")
    print(f"ğŸŒ ç”Ÿäº§ç¯å¢ƒ: {PRODUCTION_BASE_URL}")
    print(f"â° æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # è·å–æ–‡ç« åˆ—è¡¨
    article_urls = get_articles_list()
    
    if not article_urls:
        print("âŒ æ— æ³•è·å–æ–‡ç« åˆ—è¡¨")
        return
    
    results = {}
    
    for article_url in article_urls:
        try:
            seo_data = check_article_seo(article_url)
            if seo_data:
                results[article_url] = {
                    'seo_data': seo_data,
                    'status': 'success'
                }
            else:
                results[article_url] = {
                    'status': 'failed'
                }
        except Exception as e:
            print(f"âŒ æ£€æŸ¥æ–‡ç«  {article_url} æ—¶å‡ºé”™: {e}")
            results[article_url] = {
                'status': 'error',
                'error': str(e)
            }
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    output_file = f"production_articles_seo_check_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š æ£€æŸ¥å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ç”Ÿæˆæ‘˜è¦
    successful_checks = sum(1 for r in results.values() if r['status'] == 'success')
    total_checks = len(results)
    
    print(f"\nğŸ“ˆ æ£€æŸ¥æ‘˜è¦:")
    print(f"   - æ€»æ–‡ç« æ•°: {total_checks}")
    print(f"   - æˆåŠŸæ£€æŸ¥: {successful_checks}")
    print(f"   - å¤±è´¥æ£€æŸ¥: {total_checks - successful_checks}")
    
    if successful_checks == total_checks:
        print("ğŸ‰ æ‰€æœ‰æ–‡ç« æ£€æŸ¥å®Œæˆï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æ–‡ç« æ£€æŸ¥å¤±è´¥ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æ—¥å¿—")

if __name__ == "__main__":
    main()