#!/usr/bin/env python3
"""
æ‰¹é‡éªŒè¯é¡µé¢æ˜¯å¦å­˜åœ¨ hreflang çš„â€œmultiple entriesâ€é—®é¢˜ï¼ˆåŒä¸€è¯­è¨€å‡ºç°å¤šæ¡ alternateï¼‰ã€‚
- åŒæ—¶æ£€æŸ¥ HTTP å“åº”å¤´ä¸­çš„ Link æ˜¯å¦ä»åŒ…å« rel="alternate"; hreflangï¼ˆæˆ‘ä»¬æœŸæœ›æ²¡æœ‰ï¼‰ã€‚
- æ”¯æŒä» seo-pages.config.json è¯»å–è·¯å¾„ï¼Œå¹¶å¯æŒ‡å®š base-urlï¼ˆå¼€å‘/ç”Ÿäº§ç¯å¢ƒï¼‰ã€‚

ç”¨æ³•ç¤ºä¾‹ï¼š
  python3 check_hreflang_multiple_entries.py --base-url http://localhost:3001
  python3 check_hreflang_multiple_entries.py --base-url https://www.yhflexiblebusbar.com
"""

import argparse
import json
import os
import re
import sys
import time
from typing import Dict, List, Tuple

import requests
from bs4 import BeautifulSoup

DEFAULT_CONFIG = "seo-pages.config.json"
EXPECTED_LANGS = ["en", "es", "x-default"]
UA = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36"


def load_paths_from_config(config_path: str) -> List[str]:
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶: {config_path}")
    with open(config_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    pages = data.get("pages", [])
    paths = [p.get("path") for p in pages if p.get("path")]
    return paths


def fetch(url: str) -> Tuple[int, Dict[str, str], bytes]:
    headers = {"User-Agent": UA}
    resp = requests.get(url, headers=headers, timeout=20)
    code = resp.status_code
    # å°†å¤šå€¼ header æŠ˜å ä¸ºå­—ç¬¦ä¸²
    hdrs = {k: ", ".join(v) if isinstance(v, list) else str(v) for k, v in resp.headers.items()}
    return code, hdrs, resp.content


def extract_html_alternates(html: bytes) -> List[Tuple[str, str]]:
    soup = BeautifulSoup(html, "html.parser")
    tags = soup.find_all("link", rel="alternate")
    results: List[Tuple[str, str]] = []
    for tag in tags:
        hreflang = (tag.get("hreflang") or "").strip().lower()
        href = (tag.get("href") or "").strip()
        if hreflang:
            results.append((hreflang, href))
    return results


def parse_link_header(link_value: str) -> List[Tuple[str, str]]:
    # RFC8288 æ ¼å¼ç¤ºä¾‹ï¼š<https://example.com/en/about>; rel="alternate"; hreflang="en"
    if not link_value:
        return []
    pattern = re.compile(r"<([^>]+)>\s*;\s*rel=\"alternate\"\s*;\s*hreflang=\"([^\"]+)\"", re.IGNORECASE)
    matches = pattern.findall(link_value)
    return [(lang.lower(), href) for href, lang in matches]


def detect_duplicates(entries: List[Tuple[str, str]]) -> Dict[str, Dict[str, List[str]]]:
    # è¿”å› {lang: {"all": [...], "unique": [...], "is_duplicate": bool}}
    mapping: Dict[str, List[str]] = {}
    for lang, href in entries:
        mapping.setdefault(lang, []).append(href)
    result: Dict[str, Dict[str, List[str]]] = {}
    for lang, hrefs in mapping.items():
        normalized = [h.rstrip("/") for h in hrefs if h]
        uniq = sorted(set(normalized))
        result[lang] = {"all": hrefs, "unique": uniq, "is_duplicate": len(hrefs) > 1 and len(uniq) > 1}
    return result


def check_one_url(base_url: str, path: str) -> Dict:
    url = base_url.rstrip("/") + path
    code, headers, body = fetch(url)

    html_alternates = extract_html_alternates(body)
    header_link = headers.get("Link", "")
    header_alternates = parse_link_header(header_link)

    html_dup = detect_duplicates(html_alternates)
    header_dup = detect_duplicates(header_alternates)

    # æ˜¯å¦ä»ç„¶ä½¿ç”¨äº† Link å¤´è¾“å‡º hreflangï¼ˆæˆ‘ä»¬æœŸæœ›æ²¡æœ‰ï¼‰
    has_header_alternates = len(header_alternates) > 0
    # æ˜¯å¦å‡ºç°é‡å¤ï¼ˆåŒä¸€è¯­è¨€æœ‰å¤šæ¡ä¸åŒ hrefï¼‰
    html_has_duplicates = any(v["is_duplicate"] for v in html_dup.values())
    header_has_duplicates = any(v["is_duplicate"] for v in header_dup.values())

    # é¢å¤–æ£€æŸ¥ï¼šæ˜¯å¦å­˜åœ¨éé¢„æœŸè¯­è¨€æ ‡ç­¾
    langs_found = sorted(set([lang for lang, _ in html_alternates]))
    unexpected_langs = [l for l in langs_found if l not in EXPECTED_LANGS]

    return {
        "url": url,
        "status_code": code,
        "html_alternates": html_alternates,
        "header_link_raw": header_link,
        "header_alternates": header_alternates,
        "html_duplicates": html_dup,
        "header_duplicates": header_dup,
        "has_header_alternates": has_header_alternates,
        "html_has_duplicates": html_has_duplicates,
        "header_has_duplicates": header_has_duplicates,
        "unexpected_langs": unexpected_langs,
        "passed": (code == 200) and (not has_header_alternates) and (not html_has_duplicates) and (not header_has_duplicates)
    }


def main():
    parser = argparse.ArgumentParser(description="éªŒè¯ hreflang multiple entries é—®é¢˜æ˜¯å¦ä¿®å¤")
    parser.add_argument("--base-url", required=True, help="è¦æµ‹è¯•çš„åŸºç¡€åŸŸåï¼Œä¾‹å¦‚ http://localhost:3001 æˆ– https://www.yhflexiblebusbar.com")
    parser.add_argument("--config", default=DEFAULT_CONFIG, help="é¡µé¢è·¯å¾„é…ç½®æ–‡ä»¶ï¼Œé»˜è®¤ seo-pages.config.json")
    parser.add_argument("--delay", type=float, default=0.5, help="æ¯ä¸ªè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°")
    args = parser.parse_args()

    paths = load_paths_from_config(args.config)
    print(f"=== å¼€å§‹æ£€æŸ¥ï¼ˆbase: {args.base_url}ï¼‰ å…± {len(paths)} ä¸ªè·¯å¾„ ===\n")

    all_passed = True
    results: List[Dict] = []

    for i, path in enumerate(paths, 1):
        res = check_one_url(args.base_url, path)
        results.append(res)

        print(f"[{i}/{len(paths)}] {res['url']}  ->  HTTP {res['status_code']}")
        if res["has_header_alternates"]:
            print("   âŒ å“åº”å¤´ä»åŒ…å« Link alternatesï¼ˆåº”æ¸…ç†ï¼‰")
        if res["html_has_duplicates"]:
            print("   âŒ HTML ä¸­å­˜åœ¨åŒä¸€ hreflang çš„å¤šæ¡æ¡ç›®")
            for lang, detail in res["html_duplicates"].items():
                if detail["is_duplicate"]:
                    print(f"      - {lang}: {detail['unique']}")
        if res["unexpected_langs"]:
            print(f"   âš ï¸  å­˜åœ¨éé¢„æœŸè¯­è¨€æ ‡ç­¾: {', '.join(res['unexpected_langs'])}")
        if not (res["status_code"] == 200 and not res["has_header_alternates"] and not res["html_has_duplicates"] and not res["header_has_duplicates"]):
            all_passed = False
        else:
            print("   âœ… é€šè¿‡ï¼ˆæ—  Link å¤´ï¼Œä¸”æ— é‡å¤ hreflang æ¡ç›®ï¼‰")

        if i < len(paths):
            time.sleep(args.delay)

    # æ±‡æ€»
    print("\n=== æ±‡æ€» ===")
    total = len(results)
    passed = sum(1 for r in results if r["passed"])
    failed = total - passed
    print(f"æ€»è®¡: {total}  é€šè¿‡: {passed}  å¤±è´¥: {failed}")

    if failed > 0:
        print("\nå¤±è´¥é¡µé¢åˆ—è¡¨ï¼š")
        for r in results:
            if not r["passed"]:
                print(f"- {r['url']}")
                if r["has_header_alternates"]:
                    print("    é—®é¢˜: å“åº”å¤´ Link ä»åŒ…å« alternates")
                if r["html_has_duplicates"]:
                    print("    é—®é¢˜: HTML åŒä¸€è¯­è¨€å¤šæ¡ alternate")
                if r["unexpected_langs"]:
                    print(f"    é—®é¢˜: éé¢„æœŸè¯­è¨€ {', '.join(r['unexpected_langs'])}")
    else:
        print("\nğŸ‰ æ‰€æœ‰é¡µé¢å‡å·²ä¿®å¤ï¼šæ— å“åº”å¤´ Link alternatesï¼Œä¸” HTML ä¸­æ— é‡å¤ hreflang æ¡ç›®ã€‚")

    sys.exit(0 if failed == 0 else 1)


if __name__ == "__main__":
    main()