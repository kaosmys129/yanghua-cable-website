#!/usr/bin/env python3
"""
ç”Ÿäº§ç¯å¢ƒ SEO æ ‡ç­¾éªŒè¯è„šæœ¬
æ£€æŸ¥ https://www.yhflexiblebusbar.com çš„ hreflang å’Œ canonical æ ‡ç­¾
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import sys

# ç”Ÿäº§ç¯å¢ƒé…ç½®
PRODUCTION_BASE_URL = "https://www.yhflexiblebusbar.com"

# è¦æ£€æŸ¥çš„é¡µé¢
PAGES_TO_CHECK = [
    {"path": "/en", "name": "è‹±æ–‡é¦–é¡µ"},
    {"path": "/es", "name": "è¥¿ç­ç‰™è¯­é¦–é¡µ"},
    {"path": "/en/products", "name": "è‹±æ–‡äº§å“é¡µ"},
    {"path": "/es/productos", "name": "è¥¿ç­ç‰™è¯­äº§å“é¡µ"},
    {"path": "/en/about", "name": "è‹±æ–‡å…³äºé¡µ"},
    {"path": "/es/acerca-de", "name": "è¥¿ç­ç‰™è¯­å…³äºé¡µ"},
]

def get_page_content(url):
    """è·å–é¡µé¢å†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"âŒ è·å–é¡µé¢å¤±è´¥ {url}: {e}")
        return None

def extract_seo_tags(html_content):
    """æå– SEO ç›¸å…³æ ‡ç­¾"""
    if not html_content:
        return None
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æå– canonical æ ‡ç­¾
    canonical = soup.find('link', {'rel': 'canonical'})
    canonical_url = canonical.get('href') if canonical else None
    
    # æå– hreflang æ ‡ç­¾
    hreflang_tags = soup.find_all('link', {'rel': 'alternate', 'hreflang': True})
    hreflang_data = []
    for tag in hreflang_tags:
        hreflang_data.append({
            'hreflang': tag.get('hreflang'),
            'href': tag.get('href')
        })
    
    # æå–åŸºæœ¬ meta æ ‡ç­¾
    title = soup.find('title')
    title_text = title.get_text().strip() if title else None
    
    meta_description = soup.find('meta', {'name': 'description'})
    description = meta_description.get('content') if meta_description else None
    
    # æå– Open Graph æ ‡ç­¾
    og_url = soup.find('meta', {'property': 'og:url'})
    og_url_content = og_url.get('content') if og_url else None
    
    return {
        'canonical': canonical_url,
        'hreflang': hreflang_data,
        'title': title_text,
        'description': description,
        'og_url': og_url_content
    }

def check_page_seo(page_info):
    """æ£€æŸ¥å•ä¸ªé¡µé¢çš„ SEO é…ç½®"""
    url = f"{PRODUCTION_BASE_URL}{page_info['path']}"
    print(f"\nğŸ” æ£€æŸ¥é¡µé¢: {page_info['name']} ({url})")
    
    html_content = get_page_content(url)
    if not html_content:
        return None
    
    seo_data = extract_seo_tags(html_content)
    if not seo_data:
        print("âŒ æ— æ³•æå– SEO æ ‡ç­¾")
        return None
    
    # æ£€æŸ¥ç»“æœ
    print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {seo_data['title']}")
    print(f"ğŸ“ Meta æè¿°: {seo_data['description'][:100] if seo_data['description'] else 'None'}...")
    
    # Canonical æ£€æŸ¥
    if seo_data['canonical']:
        print(f"ğŸ”— Canonical URL: {seo_data['canonical']}")
        if url in seo_data['canonical'] or seo_data['canonical'].endswith(page_info['path']):
            print("âœ… Canonical URL æ­£ç¡®")
        else:
            print("âš ï¸  Canonical URL å¯èƒ½ä¸åŒ¹é…")
    else:
        print("âŒ ç¼ºå°‘ Canonical æ ‡ç­¾")
    
    # Hreflang æ£€æŸ¥
    if seo_data['hreflang']:
        print(f"ğŸŒ Hreflang æ ‡ç­¾æ•°é‡: {len(seo_data['hreflang'])}")
        for hreflang in seo_data['hreflang']:
            print(f"   - {hreflang['hreflang']}: {hreflang['href']}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è‹±æ–‡å’Œè¥¿ç­ç‰™è¯­
        languages = [tag['hreflang'] for tag in seo_data['hreflang']]
        if 'en' in languages and 'es' in languages:
            print("âœ… åŒ…å«è‹±æ–‡å’Œè¥¿ç­ç‰™è¯­ hreflang")
        else:
            print("âš ï¸  å¯èƒ½ç¼ºå°‘æŸäº›è¯­è¨€çš„ hreflang")
    else:
        print("âŒ ç¼ºå°‘ Hreflang æ ‡ç­¾")
    
    # Open Graph URL æ£€æŸ¥
    if seo_data['og_url']:
        print(f"ğŸ“± OG URL: {seo_data['og_url']}")
    
    return seo_data

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ£€æŸ¥ç”Ÿäº§ç¯å¢ƒ SEO æ ‡ç­¾é…ç½®")
    print(f"ğŸŒ ç”Ÿäº§ç¯å¢ƒ: {PRODUCTION_BASE_URL}")
    print(f"â° æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results = {}
    
    for page_info in PAGES_TO_CHECK:
        try:
            seo_data = check_page_seo(page_info)
            if seo_data:
                results[page_info['path']] = {
                    'name': page_info['name'],
                    'seo_data': seo_data,
                    'status': 'success'
                }
            else:
                results[page_info['path']] = {
                    'name': page_info['name'],
                    'status': 'failed'
                }
        except Exception as e:
            print(f"âŒ æ£€æŸ¥é¡µé¢ {page_info['name']} æ—¶å‡ºé”™: {e}")
            results[page_info['path']] = {
                'name': page_info['name'],
                'status': 'error',
                'error': str(e)
            }
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    output_file = f"production_seo_check_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š æ£€æŸ¥å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ç”Ÿæˆæ‘˜è¦
    successful_checks = sum(1 for r in results.values() if r['status'] == 'success')
    total_checks = len(results)
    
    print(f"\nğŸ“ˆ æ£€æŸ¥æ‘˜è¦:")
    print(f"   - æ€»é¡µé¢æ•°: {total_checks}")
    print(f"   - æˆåŠŸæ£€æŸ¥: {successful_checks}")
    print(f"   - å¤±è´¥æ£€æŸ¥: {total_checks - successful_checks}")
    
    if successful_checks == total_checks:
        print("ğŸ‰ æ‰€æœ‰é¡µé¢æ£€æŸ¥å®Œæˆï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†é¡µé¢æ£€æŸ¥å¤±è´¥ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æ—¥å¿—")

if __name__ == "__main__":
    main()